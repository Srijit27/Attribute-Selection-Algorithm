# ğŸ¯ Attribute Selection Algorithm - Decision Trees

## ğŸ“Œ Introduction

Decision Trees are one of the most widely used **supervised learning methods** for both **classification** and **regression** tasks.

They work by **recursively splitting** a dataset into smaller subsets based on attribute values, forming a **tree structure**:

- ğŸ”¹ **Internal Node** â†’ Attribute test  
- ğŸ”¹ **Branch** â†’ Outcome of the test  
- ğŸ”¹ **Leaf Node** â†’ Class label or decision

A critical component of decision tree construction is the **Attribute Selection Algorithm**, which identifies the **best attribute** to split the dataset at each step. The selection is driven by **impurity measures** such as:

- Entropy  
- Information Gain  
- Gini Index  

---

## ğŸ“ Formulas Used

### 1. ğŸ“Š Entropy
Entropy measures the **impurity or uncertainty** in a dataset. It is calculated as:

```
H(S) = âˆ’ âˆ‘ (pi * logâ‚‚(pi))
```
Where:  
- `pi` is the probability of class `i`  
- Summation runs over all possible classes

---

### 2. ğŸ“ˆ Information Gain
Information Gain measures the **reduction in entropy** after splitting on an attribute:

```
IG(S, A) = H(S) âˆ’ âˆ‘ (|Sv| / |S|) * H(Sv)
```

Where:  
- `S` = original set  
- `Sv` = subset for which attribute `A` has value `v`  
- `H(S)` = entropy of original set  
- `H(Sv)` = entropy of subset

---

### 3. âš–ï¸ Gini Index
The **Gini Index** is another measure of impurity:

```
Gini(S) = 1 âˆ’ âˆ‘ (piÂ²)
```

For a given attribute `A`, the **weighted Gini Index** is:

```
GiniIndex(S, A) = âˆ‘ (|Sv| / |S|) * Gini(Sv)
```

---

## ğŸ“‚ Dataset Used

| Weather | Parents | Financial Condition | Decision     |
|---------|---------|---------------------|--------------|
| Sunny   | Yes     | Poor                | Cinema       |
| Sunny   | No      | Rich                | Play Tennis  |
| Windy   | Yes     | Poor                | Cinema       |
| Windy   | No      | Poor                | Cinema       |
| Windy   | No      | Rich                | Shopping     |
| Rainy   | Yes     | Poor                | Cinema       |
| Rainy   | No      | Poor                | Stay in      |
| Rainy   | No      | Rich                | Shopping     |

---

## ğŸŒ³ Induced Graphs

### âœ”ï¸ Decision Tree using Entropy (Information Gain)

![Entropy Tree](tree_entropy.png)

---

### âœ”ï¸ Decision Tree using Gini Index

![Gini Tree](tree_gini.png)

---

## âœ… Summary

This project demonstrates how **attribute selection techniques** influence decision tree construction using **Entropy** and **Gini Index**. The graphical representations make it easier to interpret the decision paths.

---

## ğŸ“ Files Included

- `Tree.py` â†’ Python code implementing decision trees with entropy and gini  
- `Dataset.csv` â†’ Training dataset  
- `tree_entropy.png` â†’ Visualized decision tree using entropy  
- `tree_gini.png` â†’ Visualized decision tree using Gini index  
- `README.md` â†’ Project documentation